{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjQiLORit87",
        "colab_type": "text"
      },
      "source": [
        "Notebook for testing and visualising the trained models, instead of just editing in and out of the other note books. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1hCBBflpPE",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNCK1plivBa",
        "colab_type": "code",
        "outputId": "13b78cb5-192a-4c43-c654-7b502404f50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pw1B9CRiq4b",
        "colab_type": "code",
        "outputId": "c956d6dd-8c21-4920-8443-c3bd3db27596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3521 sha256=04414a4a0f0ab8f6544c1e9d753857d0772e653104092cd3358675e3d9d622a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROspCb3F4Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfGC8lSoPcJ-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HjvzMDSmjvm",
        "colab_type": "code",
        "outputId": "272224d1-af24-44d0-b0bd-90223145ed28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 0.983s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GFSfjbmn9p",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng6nuRUemmId",
        "colab_type": "code",
        "outputId": "88ba1269-3cfa-4215-98ee-1c0a243f3d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n",
            "GPUs: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ygxsDfm13g",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrqiJO2m3r7",
        "colab_type": "text"
      },
      "source": [
        "## LSTM CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABn4VwLm1No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqL4TQZm9ux",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Full Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4SSRxnrvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6r5pzTnEp1",
        "colab_type": "text"
      },
      "source": [
        "## lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f9sKamnGsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3OsS3LnJST",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliGMQernKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhOY6M2nNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaaxPlgInPbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(dataset, valid_frac = 0.1, dataset_length = 9000, avg = 0, std = 0, application_boolean = [0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_index = list(train_index)\n",
        "        \n",
        "        valid_index = list(valid_index)\n",
        "        \n",
        "        train_dataset = HDF5Dataset_with_avgs(dataset,train_index, avg, std, application_boolean)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset_with_avgs(dataset,valid_index, avg, std, application_boolean)\n",
        "        \n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXbH9ufnRUQ",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeG22ZLUnSwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtVqEhenUxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-ycpijnWaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJNW6pcnYVS",
        "colab_type": "text"
      },
      "source": [
        "# training functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id-1ba_mnaMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88roEYKncdq",
        "colab_type": "code",
        "outputId": "d7ae85ec-c3a2-4a49-bf9a-16f1e356d01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "\n",
        "    # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss / i # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, loss_func = nn.MSELoss()):\n",
        "    \n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    tot_loss = 0\n",
        "    i = 0\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad(): # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x = x.to(device) # send to cuda. need to change = sign as to(device)\n",
        "            y = y.to(device) # produces a copy on thd gpu not moves it. \n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction[:,0,0], y)\n",
        "            \n",
        "            tot_loss += loss.item()\n",
        "            i += 1\n",
        "            \n",
        "            print(\"MSE_VALIDATION_LOSS:\", tot_loss / i)\n",
        "            \n",
        "    \n",
        "    \n",
        "    return tot_loss / i # returns total loss averaged across the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        \n",
        "        \n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Qh0HFanfZd",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxMIqmhTng9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "#         print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "#         print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "#         truth -= self.avg[0]\n",
        "#         truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxJN-sRn2Vx",
        "colab_type": "text"
      },
      "source": [
        "## save fig def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHgHdoYn3qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image_save(model, train_loader, name, sample = 7, threshold = 0.5):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "\n",
        "    x = x.cpu()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(1,2)\n",
        "    print(x.shape)\n",
        "    print(b.shape)\n",
        "    axes[0].imshow(x[sample][0][0])\n",
        "    axes[1].imshow(b[sample])\n",
        "    \n",
        "    axes[1].set_title(\"truth\")\n",
        "    axes[0].set_title(\"Prediction\")\n",
        "    fig.suptitle(\"Prediction of:\" + name)\n",
        "    fig.savefig(name + \"sample\"+ str(sample) + \"comparison.pdf\")\n",
        "#     print(b[7])\n",
        "#     print(x[7][0][0])\n",
        "    plt.figure()\n",
        "    x[sample][0][0][threshold > x[sample][0][0]] = 0\n",
        "    plt.imshow(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(10,1,figsize=(32,32))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(a[sample][i][0])\n",
        "    \n",
        "    fig.suptitle(\"Preceding sequence of: \" + name)\n",
        "    fig.savefig(name + \"sample\"+ str(sample) + \"preceding.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3LLyGvaoTug",
        "colab_type": "text"
      },
      "source": [
        "## batch loss histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6Zf6jzoVwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_loss_histogram(model, train_loader, loss_func):\n",
        "    \n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "        x = x.cpu()\n",
        "#     print(x.shape)\n",
        "    # now over each one in x - we do\n",
        "        #loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = []\n",
        "        for i in range(len(x)):\n",
        "            loss.append(loss_func(x[i,:,0],b[i:i+1]).item())\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bWfVt3njrM",
        "colab_type": "text"
      },
      "source": [
        "#wrapper\n",
        "\n",
        "not put in "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1EsNMannU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deYNPMonjoJ",
        "colab_type": "text"
      },
      "source": [
        "# code imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUOBUBX2nvPV",
        "colab_type": "code",
        "outputId": "1b558900-fccb-483e-d8dd-73ae5bd55ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 5).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZFoI1Sk0On",
        "colab_type": "code",
        "outputId": "4c8186d8-68b6-415b-93c0-054f0555cdc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/masters_project/data/\n",
        "\n",
        "f = h5py.File('emerg_25.hdf5','r')\n",
        "print(f['predictor'].shape)\n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "(4940, 10, 5, 16, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFSO4IMoDo9",
        "colab_type": "text"
      },
      "source": [
        "## code loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-COzcpoJe0",
        "colab_type": "code",
        "outputId": "bb67bbf6-ab32-4a33-8f43-d09da759d13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"now changed to fixed dataset\"\"\"\n",
        "\n",
        "avg = np.load(\"fixed_25_avg.npy\")\n",
        "std = np.load(\"fixed_25_std.npy\")\n",
        "# changed below\n",
        "apbln = [0,1,0,0,1] # think this is correct\n",
        "index_map = np.arange(0,52109,1)\n",
        "train, valid = initialise_dataset_HDF5_full('emerg_25.hdf5', valid_frac = 0.1, dataset_length = 4940,avg = avg, std = std, application_boolean=apbln)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIZuUAQoNM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 2000, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMjpjcSxFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = \"bce_3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7B7f3ui0h_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgFsBi6oE2l",
        "colab_type": "code",
        "outputId": "0869f8cc-4e47-4dc8-f3e5-c8a37eca71bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = nn.DataParallel(LSTMencdec_onestep(structure, 5, kernel_size = 3)).to(device) # added data parrallel\n",
        "\n",
        "test_model.load_state_dict(torch.load(name + \".pth\"))\n",
        "test_model.eval()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): LSTMencdec_onestep(\n",
              "    (encoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (3): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7lJB7uoIO7",
        "colab_type": "text"
      },
      "source": [
        "loading in averaging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-yzx0WopWO",
        "colab_type": "code",
        "outputId": "1646cd5a-7baf-4d05-aa50-826f528470ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_image_save(test_model, train_loader, name + \"comparison\", sample = 1967, threshold = 0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([2000, 1, 5, 16, 16])\n",
            "torch.Size([2000, 16, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD1CAYAAABX2p5TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLDNO/GZmmXHiNzPLzP8Hzje1e6edqFYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKMMvNcrwS40y/FKj/g+ajMoKZIE1sQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAfRCAYAAAD1FxIiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCElNBoSkJgNCUtP/DykP27Lb4qIJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 2304x2304 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceaV2m7ZdHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def f1(model, train_loader, avg = 'macro'):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "#     print(x[0][0][0])\n",
        "    print(x[222,0,0].shape)\n",
        "    print(b[222].shape)\n",
        "#     print(b)\n",
        "    print(b[222].view(-1, 256).numpy().shape)\n",
        "    truth = set(list(b[222].view(256).numpy()))\n",
        "    pred = set(list(x[222,0,0].view(256).numpy()))\n",
        "    print(truth - pred)\n",
        "    scores = []\n",
        "    for i in range(len(b)):\n",
        "        score = f1_score(b[i].view(256).numpy(), x[i,0,0].view(256).numpy(), average=avg)\n",
        "        scores.append(score)\n",
        "        truth = set(list(b[i].view(256).numpy()))\n",
        "        pred = set(list(x[i,0,0].view(256).numpy()))\n",
        "        if len(truth - pred) > 0:\n",
        "            print(i)\n",
        "#     score = f1_score(b[222].numpy(), b[222].numpy(), average=avg)\n",
        "    return scores\n",
        "#     print(score)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKhzR5rvPov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    \"\"\"Calculate TN, FN, TP, FP for multilabel classification\"\"\"\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    # reshape\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = x[:,0,0].view(-1,256).numpy()\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fn = 0 \n",
        "    fp = 0\n",
        "    \n",
        "    print(truth.shape)\n",
        "    print(pred.shape)\n",
        "    for i in range(len(b)):\n",
        "        for j in range(256):\n",
        "            # true positive\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 1):\n",
        "                tp += 1\n",
        "            # true negative\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 0):\n",
        "                tn += 1\n",
        "            \n",
        "            #false positive\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 1):\n",
        "                fp +=1\n",
        "            #false negative\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 0):\n",
        "                fn += 1\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp/ (tp + fn)\n",
        "    \n",
        "    f_1 = 2 * prec * rec / (prec + rec)                \n",
        "                \n",
        "    if verbose:\n",
        "        print(\"tn:\" ,tn)\n",
        "        print(\"tp:\" , tp)\n",
        "        print(\"fn:\" , fn)\n",
        "        print(\"fp\" ,fp)\n",
        "        print(\"prec:\" ,prec)\n",
        "        print(\"rec:\" , rec)\n",
        "        print(\"f1: \", f_1)\n",
        "    \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'w')\n",
        "        for i in [tn, tp, fn, fp, prec, rec, f_1]:\n",
        "        \n",
        "            f.write(str(i) + \"\\n\")\n",
        "\n",
        "        f.close()\n",
        "    \n",
        "                \n",
        "            \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6iVIEUNEPR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def area_under_curve_metrics(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "#     truth = b.contiguous().view(-1).numpy()\n",
        "#     pred = sig(x[:,0,0].contiguous().view(-1)).numpy()\n",
        "#     fpr, tpr, thresholds = roc_curve(truth, pred)\n",
        "#     plt.plot(fpr, tpr)\n",
        "#     plt.plot(fpr, fpr)\n",
        "    r = roc_auc_score(truth, pred)\n",
        "    av = average_precision_score(truth, pred)\n",
        "    if verbose:\n",
        "        print(r)\n",
        "        print(av)\n",
        "    \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'a')\n",
        "        for i in [r,av]:\n",
        "        \n",
        "            f.write(str(i) + \"\\n\")\n",
        "\n",
        "        f.close()\n",
        "    \n",
        "        \n",
        "        \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDKnjygZerpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def brier_score(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    diff = (x[:,0,0] - b)**2\n",
        "    brier = np.average(diff)\n",
        "    \n",
        "    if verbose:\n",
        "        print(brier)\n",
        "        \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'a')\n",
        "        f.write(str(brier) + \"\\n\")\n",
        "        f.close()\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgdaoLwaU8j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_metrics(model, train_loader, name = 'default'):\n",
        "    metrics(model, train_loader, name = name)\n",
        "    area_under_curve_metrics(model, train_loader, name = name)\n",
        "    brier_score(model, train_loader, name = name)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWepJCa5W_sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZs5gRhzGYwK",
        "colab_type": "code",
        "outputId": "2187ab37-4ce6-44cf-8be6-f60a16512e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "\n",
        "\n",
        "full_metrics(test_model, train_loader, name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 482095\n",
            "tp: 7520\n",
            "fn: 10875\n",
            "fp 11510\n",
            "prec: 0.39516552811350497\n",
            "rec: 0.408806740962218\n",
            "f1:  0.40187040748162994\n",
            "FINISHING ONE PASS\n",
            "0.8884509501705424\n",
            "0.3439482905521257\n",
            "FINISHING ONE PASS\n",
            "0.043720703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPsmKD10Kg31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def curves(model, train_loader):\n",
        "#     dataframe = pd.dataframe()\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "    plt.figure()\n",
        "#     for i in range(16):\n",
        "    for j in range(16):\n",
        "        t = b[:,j,j].contiguous().view(-1).numpy()\n",
        "        p = sig(x[:,0,0,j,j].contiguous().view(-1)).numpy()\n",
        "        fpr, tpr, thresholds = roc_curve(t, p)\n",
        "        plt.plot(fpr, tpr)\n",
        "    plt.plot(fpr, fpr)\n",
        "    plt.xlim(0, 1.1)\n",
        "    plt.ylim(0,1.1)\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXD5JpCaLIzd",
        "colab_type": "code",
        "outputId": "5641a92d-6bbb-487f-88d5-5c4b7b080c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "curves(test_model, train_loader)\n",
        "# sns.set()\n",
        "# x = [1,2,3]\n",
        "# y = [4, 5,7]\n",
        "# ax = sns.lineplot(x, y, palette= 'red')\n",
        "# ax.plot([1,2], [11,22])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsvXl8nGd57/29Z5/RaEbLaLMkS/Ii\nO96SOI5NSAhZwVnIBiEhNCmmENrT9j1tgZb2tEDa9+2hpZyetgdoclpMkw8BhyQmmzEkQHbjeEls\nx44tydZi7dvs+zzP/f4xi0byyJIcOVp8fz+fRJ7nuZ9nbi3zm2uu+7p+t5BSolAoFIrFhWGuJ6BQ\nKBSK2UeJu0KhUCxClLgrFArFIkSJu0KhUCxClLgrFArFIkSJu0KhUCxClLgrFArFIkSJu0KhUCxC\nlLgrFArFIsQ0V0/s8XhkY2PjXD29QqFQLEgOHDgwLKWsmGrcnIl7Y2Mj+/fvn6unVygUigWJEKJz\nOuNUWkahUCgWIUrcFQqFYhGixF2hUCgWIUrcFQqFYhGixF2hUCgWIUrcFQqFYhGixF2hUCgWIUrc\nFQqFYhGixF2hUCgWIUrcFQqFYhGixF2hUCgWIUrcFQqFYhGixF2hUCgWIUrcFQqFYhEypbgLIX4g\nhBgUQrw7yXkhhPhXIUSbEOKwEGLj7E9ToVAoFDNhOpH7D4GtZzl/E7Ay89+DwPff/7QUCoVC8X6Y\nUtyllK8Co2cZcjvwqEzzW6BECFEzWxNUKBQKxcyZjZ2YaoHTeY+7M8f6ZuHeihlw+KXdvPfGy+d2\ncbAfQkO5h+FkEdFk0exMbBaQmkRqcsbXCYMEkblOF8iZ32KRIACYybcvZzj+QsFtLmNt6WVs+va9\nGKxztpndlHygC6pCiAeFEPuFEPuHhoamvkAxI95742WGOtrP7eLQECTCuYfRZBFJzTxLM3v/SE2e\nmzALiRCZe0guUHEX53SVzPu/AgwYWFtyGdcvuYMyayWpoehcT+mszMbbTg9Qn/e4LnPsDKSUjwCP\nAGzatEn91ZwHKhqbuOcb35r5hdtvAYpg2wsA7PzOQQDu/PL8WB/vvP8BABoee3RmF26/Jf112wvn\nfo9ZZP/+/Rw5cuSM4wHtTZZ4TmA3GuAsn1BeMV/BXstl03uy/NsIEELQZahlqd6DZd8wAH9y8Q4A\n9JRE0/Rxl/9nOAjA7xUVp8fI9HmDmF5MaHcMEY1U4PVtm958p6C/v5/q6mq2bZud+82EeGcA71Mt\npAajODZW4r5lGcai+RP8FGI2xP1Z4I+EED8BtgB+KaVKycwzjr7WQ8tbA5MP6P90+mtG1Ie7Q3jq\nnHh3PEHg+ednbR6pwUFSo2dbwimMHolgcDhyAj0tgv0w0gY2N/z6AWLHj2NbvZqftvyUXad2zXgO\n74fBQJzhcJxEIoGu6xgM4wVSiAQM2pgqyo5zDF28h+EcP4I4AC8ptBIDVmOC72d+FwKRfjPIe/p+\nTaPaaMyJupQSIab/KSAaqSAUXHtO8yxEdXU169evn7X7TQc9rhH4ZQehN3sxuq14tq3FtqrsA53D\nuTKluAshfgxcA3iEEN3ANwAzgJTy34FdwM1AGxABPvi3VQXs3w59mYgwG63m0XLs0wxHKvA4JkmH\nJcJgGcuxe+qcNG+uIvDI/86J4myQGh3NCfVMMDgcGMvLZ/Zk4cz3WlQBgG31aly33squU7s4MXqC\nVWWrCl42kEgylEjN7LkySD2BLs+8NhEAPQXCKMBoZKJGSiwgBSKTKZVkckgTBuoIDFJi1caeY/J0\nk0RigDMibSMmwKpZiAbrADCkTOimFBH32BuvyQDSVs5zjorcsZuX3cxdzXdP+XNYDMRavXifbkXz\nxim6ogb31sZ5nWOfyJQzlVJ+ZorzEvjDWZuR4gymtVDad4Qhv06Fe/KPzB7HEHeueWLye6z/FGwa\nn4bpfCQtirOVyui8/wEGgnG+c9OfvO97DQbjDIfi4459ipe4mdcBWLW8nxM08nm+nD5Z/Fvw/gjC\nvZBYwv6We8dda46HMCfCjJSWkzSZMKdmLvBGk5ZewJXjRTkRMmBx6lRujKUPTFBjkzBhirswJIpJ\niDjhuA+LzU5FY9MZz3FXVSn3L/HkHu/8zsHcJ618ToweT/8cyqb3xtx8SRVrP1I7rbGLGT2SxLer\nncj+AUweOxVf2oC1yT3X05oxC+dt6AImu1Ba6IWeT4XbwEWf+mO4oUBbQibdwrbfPQ8znBkjoTjH\n+gKsqXG9r/sMh+JE4ikcedHUzbzOKjo4QSMnaGQXV2FJSiwpSaTyILqlF0NsCWb/JZij43PMIhEC\nmQQJ5mSKytHh3DmzPY7Jnjj7hITASAotaSIcLD3j9DLvEJe+7Gd5qpqG5PiP9qXmEnwpL3sCz+SO\nOcs34DrlmXgbIMhOusZ+Dhlhn7g+sm33vwHwta33nX3eihzRd4fxPtOGHk5SfE0drusbEOaF2civ\nxH2BMOVCaTYVU0jYZ8DEHPtspmTyWVPjYseXrpjRNRNz5Q5LAEfmXlmKjw7Si4Pvrc3OeZi7XxvF\nHi7jWZGCeCU39tyTPmXyAnCktoTj1W5EXAMgUV5NRSjGbQf8ufuu2vgoDmc/4ZAHhFZwfjKdNMHb\ndzlD0csLjKjgOm0NpQYLXuv4NwovOu1WM8GqD+eOBYG+TPR9VhzQUtTBsxkxz3K21JNiPFowge/Z\nk0SPDGOuKcLzuXVYap1TXziPUeK+ENm/HY48Of5Y/xGofv+LTYHnnx8n6Nk89Wzw+N4uSvsChOOT\npzsKLXbG44MkkiNEkxE0qWEURgBKrenURmBAw55IR+HfchpJGQz4hw7lrh9e9TagE5CSagM0Xzz+\nTfIF918yanJTlwyCgHpTiI3uvSy99pXcGJsIMSKdHO64AkvEQsKRoDhhpyhhH3evaqObZmMpzZPo\ngisk6HEO8INLf372H9YssKpsFTcvu/m8P89CRkpJ5OAgvudPIZMaro83Unx1LcK4MKP1fJS4z3MO\nv7Sb7mPvYituzJUn0m+AxC3jFkDh0xCvGEu/TKBQTnYyZjPHns8z7/RwdzxFkdXE7ZcUzu3mL3YG\ng0HC4TBmSz8GkURoRkyYcpUmIpNVsSeTmHSdlMFAymAgbjKNNS4BIPEbSkgaXAzY1/HPjvHpi25j\nHXVaN1+O/CsGkwmjln1ZjOVZddyYwutwJ9xU16XL8QYfPkyyL4S5ZgYRXhnUXLKW7Vs+Pf1rFOeF\nlDeGd2cb8RYvlgYXpZ9ciblyZgv98xkl7vOc3EKqoXn8CUvRjCL1bPXLVKWNE9Mwj+/t4pl3CrYt\ncNnhl1l/Yu+053B3PMXyQC9lG9bRsGXppONWla1i+9bt/OP3/pHQSIj1G34JWHnt+GYcJkcu1XCs\nLwDAn2g/JJXUeML8xbE3sOTY/YY623ni4o/gc5VSk4iSmpBVqdY01oVseEMPTvk9VFfD+vXrCe3t\nI9Hux9LkpvJLG6b9M1DMPVKXhPf24f95ByApuW05RR+qQRjOrdlrvqLEfQFgK25kyaqrxhbMtv9N\n+us5LI523v/8WfPoE9Mwz7zTM+ni5/oTe6ke6qK/YnKhzqfIaiLZtBLXrbcWfNPwGl+lz7wfh97M\nPQ/vodI/iG6K4zNHANhb2cNn/GY296WrYdYlNBwWIxXaAEOpRtbXePB3pdMxb1Yv50BlAwAJWxkj\nTjeXlrnZeemmac11KgYfPgyA45KKKUYq5hPJoQjep1pJdASwriyh9M6VmMpscz2t84IS93lEoUaj\n4dMhkvHCC3gzISumcc+1cNW1WM+2SOoDHt4DkBP2Qoufna+7oGYdl55DCueVf34Cj78bhyWdP6+r\nfJcXbftBg2uLurms9C8QFUk0g06xWTISX0pj4ivcGPkqjclTdJiX4bAY8Tit+P0rGCy+Hv/goVxV\n0YHKBnqcJdSG0iWFK43pEsLJCO3tI/LO9C0xkn0hLE1unFuUR95CQGo6wdd6CLzUiTAbKb27GcfG\nyhk1ZS00lLjPI1reGiiYGzdbjTRvrpry+rOlXJ7wXEubuYRlmQaimaRUyp1WOl8fc3Ieig7RH+6n\npjdGV7WRex+fWdULgLHIjMlhIGy1ETPbMBpS6EkjRkstZfFGVnSPzc1gcFAn/Fxq+Csa9Xbay1bx\nZ2v/gYjfB0CyMv3mJ2UCy4brqGhsYjAU5RKnnZ03bp7WfCLvDM0of26ucaqofYGQ6AnhfaqFZG8Y\n+3oPJbctx1hsmetpnXeUuM8BkzUlDZ8OAZAIjQmM1IaoaGyaVnPJxEqXiaxI+viXyF5c191K4Pn3\niPl7z6nMcSQ6QlyL01El2Lduei8SY9KIKW5G6NkqhHSve9zkIGUwYCSFwVwN4iqu63+eFdEeTtiX\nYUCg6+noSkOjxbacF4uuxT84gq4lMBgt6LrEYBBYbHYc7hIA1jrtZ43UC2Gucar8+SJCJnUCv+oi\n+OppDA4z5b9zEfZ1hfoGFidK3OeA6TYlQbq+/aIrrznrmOmkXE5m0isNX0q7QwSef37GVTE9PT+m\nf+A5RkZT/GjIhN3s4C+XLWM6zej9/X3EYwmENCIMoOsSpIV/dd8PwF/Lr6NFG4m0fYyG2C5iYgWn\nDf+74L1WeuGBN38AwJKLPg9A8+aZd1fmp2JmXPWimNfEO/x4n2olNRTFcVkVJbc0YXDMb6Ov2UbI\nOfJA3bRpk9y/f/+cPPf5oFC+fCg6xGh05Iyxxu6XANDqbhh33O4vI+oe5cRHXjz7k/VnPGQy1TJv\nv/1RQiE3y/y9AAzWFvZhr6rqYsmSdgYDcb7wf08A8P/eV0JKBM/+fBlqLFEsQicuYThloMpo5esJ\nSePIYG6MQGKQeoGrM65U0ohmAF2CQcCAaQkAy4yDWCzlWC1VYzX7GYfKQp90sm+O5+SAmWFiKaPj\nkgqVQ1/g6PEU/t0dhH/bh9FtpfSuldiaZ/YJbr4jhDggpZyyMkBF7rNEoXz5aHSESCqKw2Q/y5Vj\nRN2jjNR3nH1QsB9i/rTTIdDb24TPX0Gt6OFfXvo/dC0v5ie3Fnbiu/jNAS56aphIQqN+IMbpKhsp\nEUQnjgFrwWuKjUmcxnTTkUXoJKSB/oQdkywmlfgYxYPPUZzU6DAvA8CWDGLUUwhpnGivAtJC0mgl\nYU6f8DitGDPF6sXONWPjqtenfW4yFPqkM51PNIUoFK2rVMziINaSMfryx3FesQTXxxsxWI1zPa05\nQ4n7LDLR3yPr7bF96/Zx43Y8lC6ju+cb5+D5sf0W6B+EW/8KNm3jnof3AKPcO5qO2jc/8BU+vrVw\ng0znjx4gNjjMKXc9Q5VwzRfuY6P75wXnmOXAwfsIhY7hzIhvddUnqK3N85Lb/ipwKWu3vcDR13p4\n+uWnSSY1hldcTfvFxVN+O0dDUdY67ey8dOUZ57IR+2xE6VnyF07VoujiQI8k8T1/isjBQUwVdip+\n/2KsDe/Pt2gxoMR9lggM7ic0cpgdD425LjaNpqs5duz92rix0823T0rDVbBpzFl5S1MZt/Wcgssv\np/Se8cKe385/7+hxqIRvfiodzaxx/3xa/iNO5xou2/h4+sH+7fDLPEvhPNuDlrcGSMY1zFYjRxus\ntGeE+2ycbeEzX9jPJUrPJxuxq2h9cRE5MozvmTb0SIria+txXbd0wRp9zTZK3GeJ0MhhEpF+YMWU\nY2cqVuNKHPt7SYZSaN/bzIjBxl2mYpwyScTfW3AzC9focW7LpIYqeyIM1o5vrz6b/8hjvcM8Gkrb\n4jrfbgXg7/Y8SpO/hXZ3pmPWuZxXS67mmd8cxVenoS2/BLPVyIhJmzQinwnnI2JX0frCRwsk8D3T\nRvToCOZaJ57Pr8OyRC2I56MWVKdLIbOuPL77m0oA/vDascXFbSK9wLpdTl2jfjY6H+8lOBinp1KA\n1NFTgBRETRY0DFhlCrPUEWYzwmwmqUmSmS3Tsvl0m0zvhNheVUO7RcdhMbJmydhHV2v1KWwVXeOe\n9386/4zTplrqUz3c0rWPG72vsyLaQZu9kT9e8dC4scm4hq5LdJsZIcCG5LLBTj7cf/Kcv+9zScdM\n1oykIvbFgZSSyIEBfM+3I1MarhsaKP5IXXoTlAsEtaA62xx5kqOtZbQkbyh4Oqm9gdmYLHhuNuio\nMPE/7jNjS1SgJyUIAzGjBYfVdIY1wLG+wDifc7e2mVLtagBWH3wMT2gYR9l4ywBbRRdGpw8tlK4T\nf9lyFSfMzaxKtvCn7U/y4b4jlDCMDw+pcAV/fPjXhH0JosG0dW02HROuS/uUN4gpvM+nwbmkYyZr\nRlIR+8InNRrDu7OVeKsPS2PG6Kti8Rh9zTZK3GdAS/IGhlNNeOqcuTJH4W/DEOxA6F6iTjvbaipz\n40+MetP57EkWK6eDd8cTRE5/g2S9Az1eR93w75Ho6MBYXIx19WpuX1PLfRNMuO55eA8I2LHtzM7R\nHQ89A7jGRcM9PT/m+ImfUlKyhcs+ls6tP/x2K/jCfGHdddzxsU9n/OKXUbntBSqBy0nvAKTlVQiZ\nakc5cPx1GhoauOc8bWI8lU2AitAXH1KXhPb0EvhFByAouX05RVsWn9HXbKPEfYZkK2K27d7GidET\nbN1bhU1PEa004Vs2vr57Nvy0T/50O0XAa2s1HFYT/+vEk0T27aP6oYcovWcGbf/7txN69RE+HOrB\nWlQE24/kTrlCx9iYCuBwtMChW+hPJPlqJI7LZGBdRyYy6j9C2N7ML/MshSfuALR9e/pN7HxuYjyV\nTYCK0BcXycGM0VdnANuqUkruXIGpZHEafc02StynIpNrP9paRm+wniXVY6dWla1iVVkVlDEri35Z\n8h0T7/UPotcbeXF1AzXaZuAojgJVMVMRP/BdrKGT2MvNmKySYOhY7pymRTCZXOkGImA4kU4veSx5\nHX3V62np38zw4FiknrURzqehoYFNm2bHeRHOjNRVZH5hIDWd4CvdBH7VhcFqpPSeVTguqVjURl+z\njRL3qTjyJPQf4Y3kNwF4s2g3z+7+t1nbwqyQ2Vd7pJS7+1uxyxRVoShdFRaWn7iD6yOdM9r2Lr+z\n80ZHDxQZ2d+0jIrGZWeMra76BMWZ+vW/yVTGZCtdst23WWGfuFfn+WRipK4i88VPojuI98lWkv1h\n7BsyRl/OxW/0NdsocZ8O1esZDdmIuNsZbkwLXzblou89MKNbTfQx/9xPH895or/mWc2+8uU8eHIn\nTYFeDAYDvWWC9xrgS0fSGyefci/BUBkn9PdXjbuvw+jDbkzv+fnPmdZ+y9spltg0DAYjxbEoQZud\nJa5vsmHjzPZZze++nY475VTMxF5XReoXDjKpEXipi+Br3RiKLJTffxH2tReO0ddso8S9AON8YvrT\n6Q97tAzco2zfuj0XEet7D8y4IanQ5hf9FUv54d1/ka5ySWgIo4Fuz1Ie/8xf0mH5JwAOJ76SG//1\nkTFP8yx2ox+zIU5St2IQYDYaIAUGgxGzzUbMZkNcdBMbJtlA+7HeYZ4e8BLxJWhJJaiLws5fp/Pr\nE3Pr+ezfv58jR9L5+/7+fqqrq88YM5GZ2OuqSP3CIH7Kj/fpVlLDURybqii5ZRkGu5Kn94P66RWg\n5VeHGB4Cj2OIIS3CqNFEpKiHaH1a8PM7J8+lXC9/84vO19Mi/6lru/mnN36CAyg+lK6VdzQ8gnG0\nL7PtXN7i6XY32ZZ/SFe7HD3x15SUfGSskxTY8VC6M/aer069HvD0gJejoShVQY2quMbl4bEuv7NF\n7O+8eYBB7zAes5tyimn0leV2KZoMFY0rsuixPKOvMhueL6zDtmJxGX3NFUrcCxEewmMKc+eaF9gm\nBjhhhFUV68dVvkzWXDPVHqX3nfaR1HR+/lMTMpFEj8UQJiOmP/wWXyWGERuVQ/FcJ+m4iptsI1Ve\nyz9A/8BzQDpv/n5oShm5bdcwS1aWTBmlZxn0DlMundxRfvW0n0dF4wqA6PFRfDvb0AJxnFfV4vpY\nAwbLhWv0NdsocZ8MS1Hacnb3NlYxubHWRKbaMCOp6eiZpmCZSlelCFO6KsWIjdqiFZRVW2m89dYz\nDcDyhT3PNRHALFfy+n8cAg7ljs00ZRTJNCRNJ0rPUi6dNLsbVRSumDZaOIn/+VNE3h7EVOmg4g8u\nxrpUGX3NNkrcCxAInSIU7mLHQ+/RNOojkdL5xs7/ljvvCA0QcVZlHBnH89m+LrQSjW9dF84dS6R0\nUhk7gNQVBkwGwabGUmLH02keW62FE+Ekq7CwXYaAEET+C7b/1/ibT/A5zyfi903LFreQ7zzA8EpB\nMq6xZGXJGZteZCP2yaJ0FYUrpoOUkuiRYXzPnESPpii+fimua+sRJmX0dT5Q4g5n+MaEguUkUkEg\nHcEmNZ14Qstt5hxxVjFSVdgzXcv4o+eT0nQ0CUYBJoOgxGEhNTiEHghicBVDeIhViSQ3m6fINVav\nx1vfwKmD462CQ6FjgG1aPiwTfedfK4d9ZYJuO9RhpHlz5RnXZCN2FaUrzhUtEMf7s5PEjo1grnPi\n+cJ6LDWFN5VRzA4XvLgffa2HlmcNkLglnYoBkvINLHYb93zjW2zbvY1jfQEaE5/LLYJmybfTzWFI\n4DQ52LvtqdyhnB1A3vWd9z9AZN9Jqh/6U0oj/wVmCkbkEzk1wV8dQCSr6T8cxD5Jf0d+tD6x8uXR\nt1sZyGwmfVdVKWuXnFl6poeSlEsnd9Zcq6J0xYyQUhLZP4DvhVPIlMR9cxPOK2svKKOvueKCF/eW\ntwYYjlTgcZBbpDR792Gfxu7ou07tOqOZyWGyU24vzz1+fG8Xe9tH2dJUdsb1uU7TiemXPLL7lmbJ\nCvvEqpiR997lxi9eM/n3mBF1T52Tlk0u7sw0Kk3cLGNiDfrRSDu9iWGWWDwqYlfMiNRIFO/TrcRP\n+rE0uSn75EpMnuntSqZ4/1zw4g7pksc71zwB234XgB0PPUFqcIj9n7qJWwId3CYd2OQ/5MoWs9w7\nehyA1WVa7lhsEGyrx6LbbMPS7ZfUjqukmW6naf/Ac+MidadzTcGqmLo169hww9aCOfWJ0fqdb7fm\nRH3iZhkTa9Bbo6cBWLvyoinnqlBAxujrjV4Cv+wAg6DkzhUUXV6tjL4+YKYl7kKIrcC/AEbgP6SU\n35pwfinwX0BJZszXpJS7zrjRB0ghkcvulpRDS5BMgFnE2BEEutJ14UMd7RRH4xjbTkMlGOXU28UN\nBOOMhOLgXsIR10UcyCy2HusLsKWpjPu2LKXz//x1TtRtq1fjuvXWgvfKj9YLRer5HH5pN93H3qVu\nzTqg8F6uhaL11bqRh9+KZkZEGaSXo5F2WvwdCIsRcyZFNSKCNDQ0cOXd10/5M1AokgNhvE+2kjgd\nxLa6LG305S68P6/i/DKluAshjMB3gRuBbmCfEOJZKeWxvGF/DTwhpfy+EGINsAtoPA/znTaFRC67\nW5LFkemi1JKYhYbdGiNgLKW7N5A+bimnbLCbjgor3/5MI5HOB8c1HmX55u60re32rdv584f3nNF5\nCumGpdsvGas+sa1eTcNjj5517vnR+mSRepasd0x+VUyhbtKJ0fqNJyIk+8LjukRbo6cZESEqnWN5\n9+rq6vPq8qhYHMiUTvDl0wR+cxqDzUjZvauwX6yMvuaS6UTum4E2KeUpACHET4DbgXxxl0BW1dxA\n72xO8lyZKHLp/U1XjFWUbM/sBbrtBe7JiHNN3Tv4jW9xyeM9uaqXiQI9GYXeAKZDPDFAIjFCS6YK\nZqpoPUt+1D6ZrUCWYDBEeTjEbW0H0IIJUsNRXrCZchE6pKP0mqVL2HaevNgVi5PE6SDep1pI9kew\nX1xBySeWKaOvecB0xL0WOJ33uBvYMmHMN4FfCiH+GCgCCm9XNM9ZU+PCUXec0GhfpuzRwVeuvJe7\nmwtXyZzNGbJQp+oZefZMCaZxqB3NOfarmCpaz5KN2t2VF7PzO+N9YCYSDodIJNJNSnoo3TxlcJrH\njVFRumIm6AmNwEudhF7rwVhsofyBNdjXlE99oeIDYbYWVD8D/FBK+R0hxBXAY0KIdVLKcbtXCCEe\nBB4EWLp0aYHbnAfya9j7MseyEfuENv4sq8pW5RZJG5rvPuN8vrBPthlHoU7VbJ49m1Nv3rMHuz9A\nyGnCv3TllJF6IerWrCMaWTVpNUyWPoudGuDuNVvxnWjD0uRW1S+KcyZ20ofv6VZSIzGKtlTjvqkJ\ng03VZ8wnpvPb6AHq8x7XZY7l83vAVgAp5R4hhA3wAIP5g6SUjwCPQHqD7HOc88wo4MWSo0Ab/1Bk\niJHYCMdH0+KezavnkxX27Vu38/jerlxKZ2K+fbL8erZWHSDqdnHyyivOiNTzvdgnI78jdWI1zGFf\niMpYKDe2PJHgkkSYSFe6zFHVqyvOBT2Wwv/zdsJ7+zGW2/B8cT225SVzPS1FAaYj7vuAlUKIJtKi\nfi9w34QxXcD1wA+FEBcBNmB6ht2zwNnK/4Cxlv1MNQzb0jn3x/d28cyBHjgwJs7r3ujl8iMxKgcN\nOfOuieRH7E+9+A7HQymWFvWyLnGIN36UtinQr45gcDgYPjjxRwXFJ35L87CkOKRB9fqCEXu+8+Rk\nZO0FTh4681xlLMRt77yes+DVggmWa9Uk/SEsTW6cW2omva9CUYjoeyNpo69gAufVtbhuUEZf85kp\nxV1KmRJC/BHwC9Jljj+QUh4VQvwtsF9K+SzwZeD/CiH+lPTi6ueklB9MZM7k5X/Nm6ugbfLr8r3V\ns4um9qcTLB2QlG3YWNi8awLayAj17gB/vum7WEbHFicNDgem8rH8Y9g7SsTvA6CpO4otkmBQVtLZ\naeRUxpo3n6ywT7QTyPquZwkGQ/iWpzfp+MefvwakUzDliQTV1dW5xdHBhw/n6tdV1K6YCVooge+5\nU0QPDWGqclB5/xos9VOXByvmlmklyTI167smHPt63r+PAVfO7tRmhqfOyZ0fPTTOI4Y2zkjJDEWG\n2JZJtXRYAjgawJFJp7zohVulTle1kcvOUq7Y0/NjHn3zPV7vaqTd7qLeGsBVtYnLbpo8Z77joa/l\nBDsln8EL/EbcPun4yXzis77ra53pTr9wOIQuNQxiLIKqSUS5JBFm/fr1uY5T5aGumClSSqKHhvA9\ndxI9puG6YSnF1yijr4WC+ABCSmkTAAAgAElEQVQD7HFs2rRJ7t+//5yuzc9Hh30JfIMRrHYTHlM7\nJMI5j5gczgoormaoo51RV4LdWwb49PESGvakUzmOvI+Wnu4go6VmmusvGbu8uIOi4u7c43hZmI5Y\nNXHNis0Qx2UKU1NRkttguhCDnacAqGxYNs7dMRuJB4MhwuHQpNdn6bPYqUlEebA//ZGkv78fU8rJ\nCseVBT3YJ0bsKh2jmA4pfxzfzjZix0cx1xdT9smVmKuV0dd8QAhxQEo55S70C3J5Oz8fHc14kNuL\nLRAlLew1hcv5KhqbOFXSwaqyVdzZrjE62E9/xVJWl40thB7nOEWBxLhKl6LiboQ1RKtehzRI9Hgx\ncc2KQ0+xIh7EWF6O2VJJyDtKJOAr+NzJWBSzLeOrkbeQm43EyzOlihbL2euDqyIRVvQNMDyQfiMw\n4aQy7GFVIlpwByQVsStmgtQl4X39+He1gy5x37IM55VLlHXAAmRBijuM7YSUre++88sb85qSxvLU\n45wbg/2cCPewqt8C/RrWkhRLrxumoSaZGx8dHaGyJ4zD7MC2eiR9sD/FUdnMZxJ/w9LiHlKpAI7S\nJu7efCnrtoyVdL7w0NcY6pxkEdSe7iKtzGs2eqx3mD2+ME2xYG7xc6oGop3fOchwN3jqlueOrUpE\nKZ7kA5jKsSumS2o4Y/R1yo91uZvSu1ZiKldGXwuVBSvu02Vcs9E43/RAbsxgdJDR6CgAkVTkzJtU\nr+cN30bWlLv488vTDo6XbfyTgs83HU/1LNmF0abeznENRJNtqAGFN6vORuwqOlecC1KThN7owf/L\nToRRUHrXShyXVynrgAXOghb3o6/10NvqY8nKknSzUufr0HAVcGYX6fat22H7LXiPBgiMNhDzHafT\n3cAPy/8CR8MjnBg9waePl3DRgWEs/Ul+s+l2Xk98PF3lEvDRqzuptR3D59tL0lfBjucmr3CZimye\n/WgoSlMsyI2G5LiIvVD1T5azbVatUMyUZH+Y0SdbSHaHsF1URukdKzAqo69FwYIT93w/lWx027y5\naqxKJpPLHtdFaihJp2z6jxA4Vk7Ml86nH3GN2dhm8/CxwWFsa9bzcv1GWvoCVCR8JGIxltjgQ9Xp\nBeDYUOHu2skqXCby9ICXw74IFYEI9f1tDA+GcuklKBydKxSziUzpBH5zmuBvTmOwmyj7zGrsGzwq\nWl9ELDhxz3dBPHmIsT0/20hH7ZvGIuD8iD1XoVKUxFZdTcNjj/LSf/wDfuM/sSV6kg8VG+jeKolt\n1UnaWwi8d5pqM/xx8yNYbHYqGpdlDL22cP2XZm4TkCWbZ18ZlNy0by8Jk49S6/h0ytmi84mbaQDj\n/NcViqmIdwXwPtVKaiCC49JK3Lcuw1hknvpCxYJiwYk7pP1UjNb19LaeSKdkJvDTlp+yf2A/m6ry\nqoUypYfJHTeh9Ryn8/4H+IP+o+jEcf9OEmHT0SMG4nYrqVg0d5nFZsfhTj/HdA29CpHNoz+yUkCx\n4KJTMcxWIzV1DWzbdte07zNxMw1Qi6aK6aEnNAK/7CT0Rg9Gl4Xyz63FvvrMHcIUi4MFKe7A+JTM\nBLLVMfnpmGwj06A3zKihmO+VfpRY8WYMxTrGfh1DlwnraBF+o0BYzAxZPKypcbH1rj88p/lN7CQd\n9oZILtUZsBvTUbtmpTezlV+haHwyVGmj4lyItXnxPt2GNhqj6EM1uLc2KqOvRc6C+u1O3HUol5LJ\nIz9qv7unZUzYM7n4EYOVmMFEa7lEQ1BvFRgBW0k9lspKRKbZqJCH+9mqWABGEh14U2lPtR3r1zNU\n5KQi05gkdRBmqE4ZWRf34isbIdA/isNdXTAanwwVpStmgh5N4d/VTnhfPyaPnYoHN2Bd5p7raSk+\nABaUuGfz7bHIMgK+wtUkuai97xT0t4+ZhpH2WEdoWI0af375vwLQaNcoc1+cK23c8dAzANzzpc+e\nce+zVbEAeFM9RDU/dmP6xVMRDnHPkSO5845iCw73WJNSRVEZjb4ykn4VjStmn+jREbw/a0MPJXB+\ntA73DUsRZmX0daGwoMQdwFbcSCKxGt1t4uVYmJ9k9iqlP+3T0sEvuChp5Oa+Vo6al/GGbyO/yoyJ\nHw/zWYMVq4hj09MuxmVudy6PPvGTwcRIfaoqlu3bDwFOtm3bxpsZP/Wv3nHNpN+LMvNSnA+0UALf\nsyeJHh7GXF2E53fXYKlTRl8XGgtO3CFdTfJ994v0pd7EYTFBKgGVUTCYiIl0M1KHeRl/W/5tEolB\nQv5DpPQU5lINqzFOkSXCj3r+PwDuv3Vsl6WJ+5FOjNQLVbHs37+fI5novL+/P2exezaUmZfifCCl\nJPLOEP7nTqLHNVw3NlB8TR3CqIy+LkQWpLgD+I1vYTD1saZ6bTqvrvmhbAUUr+XmvlMsGdD5x/e+\nTzD0HslUEC0JlqTE6BMM1LgKbq4BnLEf6VT15keOHMmJ+nS3qcsXdhWxK2aDlC+Ob2crsRNeLEuL\nKf3kSsxVyujrQmbBiHs2ZWIrbswds8n6sTp2axXcnc6ts/0W2o92EB31otXpJKURc7/AHNPodVZw\nYPlNrKlxcaOhPZdjh+l3mE5kOp4wWUJ7+0i0+9U2d4pZQeqS8Ft9+Hd1gJS4P7EM5xXK6EuxgMQ9\nmzJxlm9gKDpExNCCQ2+edLyup0jWGUl98xJeHB5l07+MoseX8MO7/4IdX0qnYnY89Mw4Qc/vMB1n\nbTAFbznLeXbCnqX5nuv5ZEseVcSueL8khyJ4n2ol0RHAuqIkbfRVZpvraSnmCQtG3CGdMvHJdXT7\nT0I9uLXN4wdkN8PuPwLYMRqLaHHeyU+O/C2bKOxuN5nR19nq6CEdge978y06/Z28fckyRkbCrIqP\nnW8GbuyJMPjWeBveZJ/a5k7x/pCaJPR6N/4XuxAmA6WfWonjMmX0pRjPghJ3gOFQAl1KHHozn1v/\nGQC87wQIHAsB/5rZrKOcxEgM3RzG9aff5hvJFLUjSU6Xjt1nYmVMPvlR+8Q6+iyRd4Z43BFnf+NV\n+J3FrInDD7qnXrhSeXbF+yHRG8L7VCvJnhC2teWU3r4Co+vsewAoLkzmtbjn77iUnz4xGARralzc\nl/FSDxwZJTakYSvTwVJEvKyKuK2dRCNwCorNxUTqm3jWNpbGmVgZk8+e1/fiK+tEpCw88vdvFpyb\nTGgcvHgdXpebS8uKuauqlMpbPLP1rSsU45ApncCvuwi+3I3BYaLss6uxr1NGX4rJmdfinr/jUjYf\n3ra7wEAtia1Uo+Fzq2H9p9gd+xHmRArtl0U89FkTHxpaSvGxMmyxAa5qeSKXa59YGQPpqH0g2Ilm\nCWPSzMiEhpiww/v+yioOeyoZcRaxwmxk56Urz+NPQXGhE+8M4H2yhdRQFMfGSty3KKMvxdTMa3GH\nM3Piz+1+efyA/dtBT4HBBNtewLvjCUyx0xjjgtQRG6uuWcWyQwbCoQFwVlHpsubuWyhqz+baPfZS\nbvZuKFjV8vO3WxkNRbnUaeeuqtIz7qFQzAZ6XCPwiw5Ce3oxuq14Pr8OW7P6e1NMj3kv7vk8vreL\nQDSJxZF3MOvjbkxHMoHnn8f8EZ2k1cjmB77Cx7d+mh17v8aQ08rxjffz0JeuOPPGGZ77yUscGzmS\ni9phrKol3wgsWwmjInbF+SLW6sX7dCuaN07RFRmjL+uCerkq5pgF9dey96VOmjUjI0YD8fggBw7e\nB1WdlJgMQIrdr9+A6YbTUKEzaCmm9IZP5xZOKSm8wUY+x1vfQzeFqDC4WZaoHFfVkt05aa3TzloV\nsSvOE3okie+FdiIHBjBV2Kn4/Q1YG5XRl2LmzEtxzy6k5i+iPr63C1tvDDASbOwkkRwhFOoh38Ir\nPthPUVQjPmTCuf46YGzhdKRq7RnPc/S1Hva8vjfn5BhJ+fEIF7fpmwpWtahoXXE+ib47jPeZNvRw\nkuJr6nFdvxRhVtYBinNjXop7vrBn8+LPvNNDI5Aq8dHueRZNC9PYAw0nTtOpecDqwPafZjwnNd65\n9Q6Ov+nnAP8NR2iASMlSXjGsYM2E52l5a4DBcBcpUwi70Y3D5GaFcQnmcuX3ovjg0IIZo68jw5hr\nivB8bh2WWrWzluL9MS/FHQo3FxVZkmAY5kQ4zKqiIqo7OwFIGM2MCo1IKkXX8mKOB/3YAv3EXNVE\nnFWMVK0d58+edXvsHDhBwuGjoaEhZx8w+PD4piOF4nwhpSRycBDf86eQSQ3XxxspvrpWGX0pZoV5\nK+75fO8XT7K33c4aqRPU03/497iuwBp5D6wuTjlcRFJRHCY75fZyAGKuah56+HsF75d1e0yWDYMG\njb6ynKhnDb0m7qQ0mZ2AQnEupLwxvDvbiLd4sTS40kZflY6pL1QopsmCEPcX3g0AdkpsScIGc26X\nJW+bg0Cvi8rRCIO1DlaXrZ7yXvndp6YyJ+4+QbO/AjKvq2yuPX8BFVCLqIpZQeqS8G/78O9uB6Dk\ntuUUfahGGX0pZp0FIe71fidb4gKz7iJszew1Guwj0GknFoDBOgfvXeZhdfvYNastywumWER3kCud\nRgbi7Rzq7KSG0jM81R/rHWbPiTBXlBSpBVTFrJEciuB9spVEZwBrcymld67AVKqMvhTnhwUh7kuC\nTlxxC4HSQVrdB7jbUAKjJ4FybE21/OT3axkMxFn1ZgCASL2d5c6GSfcltdhNdNIHQLO78YyqmGw6\nRkXqitlAajrBV3sI/KoTYTZSenczjo2VyjpAcV5ZEOIOELAmaP/IixRBeuNrALMditM7Hw2H44Tj\nKRIuAx82elhirqLFPUSH5cS4+wxb0htWp0SIhoYGrt12e+5cNs9+NBTlipIi7l+ivGIU749ETwjv\nky0k+8LY13souW05xmJl9KU4/0xrWV4IsVUIcUII0SaE+NokYz4thDgmhDgqhHh8dqcJmgixf2D/\n2AGrC4zjXyRFVhPJYgMNRemix5PGfvr7+wver9CuSfl5dhW1K94PMqnj393O4HffRgslKP+diyj/\n7EVK2BUfGFNG7kIII/Bd4EagG9gnhHhWSnksb8xK4C+BK6WUXiFE5WxNsKfnx0hZhGaMAHDzspuh\n5X9CPMAQpYyMHufEqAkY80e3FhVhaXBjtFioLh6/S9LO7xwE4M5thbfOU41KivdLvMOP98lWUsNR\nHJdVUXJLEwaHMvpSfLBMJ3LfDLRJKU9JKRPAT4DbJ4z5IvBdKaUXQEo5OFsT7B94DgAhRLpKpvlu\nCA+RjBowDqbwdAf5xo9S/I8fjVA91HXWe2UrZRSK84EeT+F9po2hfz+M1HQ8v7eOsrublbAr5oTp\n5NxrgdN5j7uBLRPGNAMIId4AjMA3pZRnmPMKIR4EHgRYunRqrxeAgfdW4Yk56Tf70wf2b4d4AC1h\nw5KQJKxGVpet5lhfgP6KEuqrr8Ctlxe811S7KykU50qsJWP05Y/j/PASXB9vxGA1Tn2hQnGemK0F\nVROwErgGqANeFUKsl1KOC5OllI8AjwBs2rRJTufGw20rAGj3tGCGnAtkeMhJwmpksNbBpY89yre/\n/l3KB45yqdXN85YDmOJ2hkZHqa5OL7hOZ3clhWKmaOEk/hdOETk4mDH6uhhrg2uup6VQTEvce4D6\nvMd1mWP5dAN7pZRJoF0I0UJa7Pe9n8n19PyYkTAMWn0cqv4Zm9hEcmgQzW9j6B0rtpSWG1s+cBRH\naICeqiA+Q4wlxW6qi8cWTVXUrphNpJRE3x3G98xJ9EiK4uvqcV2rjL4U84fpiPs+YKUQoom0qN8L\n3DdhzM+AzwDbhRAe0mmaU+cyofy9TfsHniOQuBdhSn8AuNlQgjnSQjJhQRhBpqCzzs2Oh76WNghz\nVlFqs1OBnc0rPkbLWwOcfgVOv3KQ4e4QJz9UwkvOCLzdWvC5lcWAYjpogQTeZ9qIHR3BXOvE8/l1\nWJYooy/F/GJKcZdSpoQQfwT8gnQ+/QdSyqNCiL8F9kspn82c+5gQ4higAV+VUo6cy4SyFr3uyos5\n+tx6XAkrw84om6o28dG21wFo9zvorhREDEb6bU4SHe05g7Ba0tmerH+Mpy79ovPUOTnaYKX9LAKu\nSiAVZ0NKSeTAAL7n25EpHfdNjTivqkMYVTOSYv4xrZy7lHIXsGvCsa/n/VsCf5b5731Tt2Yd0cgq\nIiMDjDj6aPUc4O5lNzN69FWE30r3QFqwq/Rymmz3YS4yc8gSY4B+hhMhPJYSIC3od345XfL4WO8w\nh050c4VTWQooZk5qNIb36VbibT4sjRmjrwpl9KWYv8zrDlVH+Shv1TxCiR7mE//5Ekazl1jSzGrS\njSDhZClWWcZI3EufqY+IIUyVpYS1Ky9icEJVpLIUUJwLUpeE9vQS2N0BQlByx3KKNiujL8X8Z96K\n+1B0iN5wHJ8pSHUigR6JEDFb6PPZWFLnx9tRiympc1Cc5F1LB7rRgbPUw5WXfiKXknnvYiePZvLr\nylJAMVOSA2G8T7WS6ApiW1VKyZ0rMJUooy/FwmDeivtodISk1KhPGLjKZ2LI9zG+ef0xSITZbl7G\ncHu6KuGUaQCj2Uzj0lrWr19Pyytjufb8HLvKpyumi9R0gi93E/h1FwarkdJ7VuG4pEIZfSkWFPNW\n3AGENPONISPNoSAD2YOWIk7wp4jkvxGXCeJJPzazI10dkyfsd355I4++3cpaUDl2xbRJdAfxPtlK\nsj+MfUPG6Mup/GAUC495Le6TkWoJYQZCRNEdThyu4nHVMaqWXTFTZFLD/1IXoVe7MTgtlN+/Bvva\nwp3OCsVCYF6LuxmNDdpJYpQzFB1i7amr+GjkMuwGJ2GZIJzZa/JDV3+U06+Mr45RKKZL/JQf79Np\no6+iy6tx39yEwT6vXxoKxZTMy7/gocgQQRnEQTrH6X03RSzQx8d7dVKG9+gjgTM4gqm0FHtZNZs2\nbeL0KwfneNaKhYYeS+Hf3UH4t30Yy2x4vrAO2wq1LqNYHMxLcR+NjYIVXBocpgnzyTh2TzGmwBDB\nEhdJgyRWXU14zUVc/+FN43xjFIrpED0+im9nK1oggfOqWlwfa8BgUUZfisXDvBR3ALvBSImmI3V4\neV09V/UKjO4yfnv1xfQ5y/n2X/333NisR3t+rv2x3mH2+NL7oCoUWbRwEv9zJ4m8M4Sp0kHFH1yE\ndaky+lIsPuatuAMIBLomiAkjAp2kjCENAWBsoWsyt0fVtKTIR0pJ9PAwvmdPokdTFF+/FNe19QiT\nMvpSLE7mrbgbdI0iGUcCTcNejKN+DJ56gqbiceMmuj2qfVAVE9ECcbw724i9N4q5zknFF9djrlaf\n6BSLm3kl7llHSL1SIvS0AdiLiUu5Pno8fb6xEU1Y8Tit467Lj9rVPqiKLFJKIvsG8O06hUxJ3Dc3\n4byyVhl9KS4I5pW4Zx0hB+tD1A9CCBtPys3cau7GV2GjbXUjy2prxm1s/Vo57CsT42wG1D6oitRI\nNG30ddKPpclN2SdXYvIoO2fFhcO8EneAiuWVdFGFK9SMbj6FUxRh1u1g8FNdWzNus+ujr/Xwhl1j\n0G4im3xREfuFjdQloTd6CPyyEwyCkjtXUHR5tTL6UlxwzDtxTyRGqB68CYDTlhA3ShdDVsGgzcUj\n1St4Nm+jjWFviP4SI6tNFhWpK0j2hxl9qpXk6SC21WVpoy+3deoLFYpFyLwTdwCEwFfUyi+SNv4f\nxzADVp2EIV2DHPEliAQTACTjGksx8jvrld3AhYxM6QRfPk3gN6cx2IyU3bsK+8XK6EtxYTM/xT1D\nsMjFty+v5s/efRGr1Pm3CjunX4nn7bBkoHlzJWtVRcwFS+J0kNEnW0gNRLBfUkHJrcuU0ZdCwTwU\nd38qRcgCDiDqcON1urBoGrWuopzNgPKQUegJjcCLnYRe78FYbKH8gTXY1yijL4Uiy7wUdyzg0gS6\nwYAn7McTDlHlUdYCijSxkz68T7eijcQo2lKN+6YmDLZ596esUMwp8/IV4dQlJZoBicQgBXFjelFM\nechc2OixFP5d7YTf6sdYbsPzxfXYlqu/BYWiEPNG3LMNTJTrAEhga8tvufH1Q3hGB6Cu4oxuVMWF\nQ/TYCN6ftaEHEzivrsV1gzL6UijOxrwR92wD01B1kPpA+thH2w9SF+ilv2IpjbfeCt2c4SGjWNxo\noQS+504RPTSEudqB5/41WOqLp75QobjAmTfiPhQZIlyWIGDdgDvUDOaTAHTXLOXxO/6Cm+65Ar6j\nPNsvFKSURA8NpY2+4hquG5ZSfI0y+lIopsu8EffR2CgRqXPR8GUAeIIHaRo8SWvTaja0xtn5nYN5\nJZCKxUzKH8e3s43Y8VEs9cWUfmol5ipl9KVQzIR5I+4AtiQsS6Wwm9+l/uhrAAQbNuOMpvPwan/U\nxY3UJeF9/fh3tYMucd+yDOeVS5R1gEJxDswrcQcQmgHQGDaUEGqqYI3zw+xCqrr2RU5yOIr3qVYS\n7X6sy92U3rUSU7ky+lIozpV5J+6aNBKMmlg9MMLoEg9HNY1Bj3mup6U4T0gtbfTl/2UnwiQo/eRK\nHJuqlHWAQvE+mVfiLo0gBRji6TSMo3w9z622c/slqjpmMZLoC+N9qoVkdwjbmnJK71iO0aWMvhSK\n2WDeiLs5ZscWcxE21uKkmxONzdQtu5EdX7pirqemmGVkSifwm9MEf3Mag91E2X2rsa/3qGhdoZhF\n5oW4H35pNxZfAEw1FJl6qPQfIIZ6oS9G4l0BvE+2khqM4Li0EvetyzAWqbSbQjHbTKtoWAixVQhx\nQgjRJoT42lnGfVIIIYUQm2YyiWwDU6y4ipWlP6DG9yYACZMS+MWCntDwPXeSoe8fQsZTlH9uLWX3\nrFLCrlCcJ6aM3IUQRuC7wI1AN7BPCPGslPLYhHHFwH8H9p7LRHRrOVHncuAlhAAhBAn1ul8UxNq8\neJ9uQxuNUfShGtxbG5XRl0JxnplO5L4ZaJNSnpJSJoCfALcXGPd3wD8AsXOdjFlqfMjwHmRyr28X\nqW7EhYweTTH6ZAvD//EuwiCoeHADpXesUMKuUHwATOdVVgucznvcDWzJHyCE2AjUSylfEEJ89dwn\no+Ftc2DsT6E3wW9dStwXKtGjGaOvcILij9bhumEpwqyMvhSKD4r3HUIJIQzA/wI+N42xDwIPAixd\nurTAAOjpKsOIzr5LVJXMQkQLJvA9d5Lo4WHMNUV4fncNljpl9KVQfNBMR9x7gPq8x3WZY1mKgXXA\ny5lStmrgWSHEbVLK/fk3klI+AjwCsGnTJjnuXO6r4J2VF0Hj1XlHFfMdKSWRtwfxP38qbfT1sQaK\nP1qHMKpPXwrFXDAdcd8HrBRCNJEW9XuB+7InpZR+ILeJqRDiZeArE4V9WmS03JmS3NCXpLVG5WYX\nAilfLG30dcKLZWkxpZ9qxlzpmOtpKRQXNFOqp5QyJYT4I+AXgBH4gZTyqBDib4H9Uspnz8fEno5F\nSX8oUMxXpC4J7+3D//MOkBL3J5bhvEIZfSkU84FphcZSyl3ArgnHvj7J2GtmPItgP/kpGCklz5Lk\n75XtwLwlORRJG311BLCuLKH0zpWYymxzPS2FQpFhfuQ9QkPA+J3rtzSVcd+WAouuijlFapLga90E\nXupEmIyUfqoZx2WVyjpAoZhnzA9xB0AglUDMaxK9IbxPtZLsCWFbW07p7SswuixzPS2FQlGAeSHu\n4WQRJs1OIvPYnVBVMvMJmdQJ/LqL4CvdGBwmyj57EY71nqkvVCgUc8a8EPdoMr2Fmr/ktwCUJrS5\nnI4ij3hnAO+TLaSGojg2VlJy6zIMDuULoVDMd+Zc3J/9z38nFh8iaS2ntm8PpkEjiQrVyTjX6HGN\nwC86CO3pxei24vn8OmzNpXM9LYVCMU3mXNzb96Wj9URRLVceNQM6sYbNczupC5xYixfv061o/viY\n0Zd1zv9UFArFDJgXr1jNWo6xqIESXdDWtJq2i6+Z6yldkOiRJL4X2okcGMBUYafiSxuwNrrneloK\nheIcmBfiDlAcAtOghmyUyjBsDoi+O4z3mTb0cJLia+pxXb8UYVa/B4VioTJvxL0okv56YMOWsw9U\nzCpaMIHvmTai746kjb4+tw5LrXOup6VQKN4n80Lc09XtklSlEVPtZo71BVhT45rjWS1upJREDgzi\ne+EUMqnh+ngjxVfXKqMvhWKRMC/EPWs9MIKbptMB1qys5nZlPXDeSI3G8O5sJd7qw9LgovSTK5XR\nl0KxyJh7cdcS6WkIwSAl7K44yI4v/d1cz2pRInVJeE8v/l90AIKS25dTtKVGGX0pFIuQeSDuSWCs\nKWawXOV7zwfJwYzRV2cAa3MppXeuwFSqjL4UisXKnIr74Zd2kwpHMQsHFiEx6Dql2tVzOaVFh9R0\ngq92E3ipC2ExUnp3M46NyuhLoVjszKm4v/fGywAI+6UkNMFQWcVcTmfRkegJ4X2yhWRfGPt6DyW3\nLcdYrIy+FIoLgblNywT7MQkPJusG3ql/groTynZgNpBJjcCvugi+2o2hyEz571yEfZ0y+lIoLiTm\nVtxDQ0AFtmgrvZ491J3YOKfTWQzEO/x4n2wlNRzFsamKkpublNGXQnEBMucLqgIJMl0KGTVH53g2\nCxc9nsK/u4Pwnj6MpVY8v7cO20pl9KVQXKjMqbgHNTNjNe4uoubwXE5nwRI7MYp3ZxuaP47zyiW4\nPtaIwapSXArFhcycintYM2MAEIIhUcJQ9fK5nM6CQwsn8b9wisjBQUyVdip+/2KsDaqzV6FQzLG4\nm6MxUtKOhgQE4Zp75nI6CwYpJdEjw/iePYkeSVF8XT2u65YiTMo6QKFQpJljcU+SsoLfIdBCl0DR\nXM5mYaAFEnh/1kbs2AjmWieez6/DskQ1fikUivHM8YKqQBcQLxJo4Q8pcT8LUkoi+wfSRl8pifum\nJpxX1SKMqhlJoVCcyZyKe9LkRDemDasEKqUwGanRGN6nW4m3+bA0uSi9ayXmCmX0pVAoJmeOxT0d\nqg+UHgR96VxOZV4idaeOJDsAAAyKSURBVEnozV4Cv+gAg6DkjhUUba5WRl8KhWJK5rzO3aBF6PUc\ngsE72Ns+ypamsrme0rwgORBOG311BbGtKqXkzpWYSqxzPS2FQrFAmHNxz27VURRLkUBc8D7uMqUT\nfKWbwK+7MFiNlN2zCvslFcroS6FQzIg5FXeBRGaamFyRFM2rq7hvy4Wbnkl0B/E+2UqyP4z94gpK\nPrEMo1MZfSkUipkzx5F7pjtVd8/tNOYYmdTwv9hF6LVuDMUWyh9Yg31N+VxPS6FQ/P/t3XuMVPUV\nwPHv2Zl9784+AZXl4bIssghVpD6aRrRgRYOilVa0RFupqK1NG/8yMTGN/aePtE2b0BTSGqtJq1WM\n3RaI1gdqqbysCkIV1gVkeS77fszuzuP0j3tt1g0wFzozd2b2fJJJ7sz89s45O7Nn7/7u757NYr4v\nhQRoD1RyXXiYU/4G44vh1m661u8n2jFE6ZUXUHHTxeQVZ8BsmTEmq2VEFQlpmPnhYV7xO5A0ig9F\n6dl0gIFtxwlUF1H7nbkUNVT6HZYxJkd4WlwuIktE5GMRaRGRR0/z/CMisldEdonIayIyLdE+w729\nRLQDUMoGBynr+/g8ws9O4Y86OfGrdxnYfpyyL09m0g/nW2E3xiRVwuIuIgFgDXAT0ATcJSJNY4a9\nByxQ1XnAC8DPEu13qL8PgGBgMjOOVhLq3XOOoWef2ECEzmc/ouOpPUhhkAkPfYHKpfXkFVgHR2NM\ncnmZlrkSaFHVVgAReRZYBuz9bICqvjFq/FZgZaKdxmJxJFhHcXA6TZ8eAXrPKfBsoqqEd7U7jb6G\nYpQvmkro+inW6MsYkzJeivtk4PCo+23AVWcZvwrYdLonRGQ1sBqgrtr5t2+lfTsZyIfds862y+wV\n6xl2Gn39p5P8ujImLG8k/wJromOMSa2knlAVkZXAAmDh6Z5X1XXAOoCptRM0LzZIqHcn7dWz+W35\nvLP+xsg2qsrAjuP0bDgAcaXiZrfRl7UOMMakgZfifgSYMup+nfvY54jIYuAxYKGqDifcq/uv9SIE\nyIs6UzK5cnVqtCNM1/r9DLf2UFhfQdXXZhKsLfY7LGPMOOKluO8AZorIxThFfQVw9+gBInI5sBZY\noqonvb+8EMH5581XXVyd9Venalzp33KE3lcOOY2+bm+g9IvW6MsYk34Ji7uqRkXkYeBlIAA8qap7\nROQJYKeqNgM/B8qA590eKJ+q6q0pjDvjRI4P0Ll+P5HDfRRdUk3l7Q0EK6zRlzHGH57m3FV1I7Bx\nzGOPj9penOS4soZG4/RtPkzvG4fJKwpQfdcsiudZoy9jjL98vEJV/XvpJBk53EfnC/uInhik+LIJ\nVN4yg0Bpvt9hGWNMZrQfyDbxkRi9rxyif8sRAuUF1NzbRPFsa/RljMkcvhf3bDt+H/rEafQV6xyi\n9Cq30VeR799GY4z5HF+rUp4q2TIzHR+K0rPxAAPbjxOoKaL2/rkUzbB+MMaYzORfcXcP2d+eVU7j\nMd+i8CS8t4Oul1qI941Qdm0docVTrR+MMSaj+XrkHhfh1XkVNB4L+xnGGcX6R+j+WyvhD9rJv6CE\n2nuaKKgr9zssY4xJyMfi7kzIBGKxjJt3V1XCH7iNvoZjhG6YRvnCOmv0ZYzJGr6fCQzEovQUBzKm\n9UC0e5jul1oY+qiTginlVC2fSf4ka/RljMkuvhd3FRgsr/G99YDGlYHtx+nZ5Db6WlpP2ZcustYB\nxpis5HtxByGo/v6D7Mgpp9HXyIEeChsqqbq9gWCNNfoyxmSvDCju0DFpji+vqzGl/59H6PnHISQo\nVN0xk5IFk6x1gDEm62VAcVfaJ89P+6uOHBuga/0+Im39FDXVUHXbDAIha/RljMkNPhd3pTQ2CGls\nx6LROL2vf0rf5jbySoJU330JxXNr7WjdGJNTfCvuny1/vOxgnFOXpOc1hw/10rV+H9GTYUoun0jF\n0npr9GWMyUk+H7kLcw6FeDPFxT0+EqP35YP0/+sogVAhNd+eQ/Gs6tS+qDHG+Mj3OXchtZfxD+3v\nouvF/cS6him9+kIqlky3Rl/GmJyXs1UuHo7SvaGVwZ0nCNYWM2H1PArr/V1yaYwx6eJfcXdPYA4U\nh5K+6/CeU3S99AnxgRHKr6sjtGgqkm+Nvowx44evR+7BWD9dFZOStr9Y3wjdzZ8Q3n2K/AtLqb3X\nGn0ZY8YnH1v+RpDIUHJ2pcrgv0/S/fdWdCRG6MZplF9bhwSs0ZcxZnzy9ci9bDDGyf9zH9HuIbpe\nbGF4XxcFU8upWt5I/sSSpMRnjDHZysfiHiQ0oOfdekDjysC2Y/RsOggolbfUU3qNNfoyxhjw+chd\nVHkzr4Gmc/y6SPug0+jrYC+FMyupun0mweqilMRojDHZyN+lkBqn6cKQ517uGlP63m6j99VDSDBA\n1fJGSq6YaK0DjDFmDN/XuT/3wDWexo0c7adr/X4iR/opnlND5W0NBMoLUhydMcZkJ9+LeyIacRt9\nvXmYvJJ8qr85m5K5tX6HZYwxGS2ji/vwwR661u8n2h6mZP5EKpfWk1dijb6MMSYRX4u7nmGqPD7s\nNvp65yiBikJq77uUosaq9AZnjDFZLOOO3If2uY2+eoYpu+YiQjdOI68w48I0xpiMljFVMz4YoXvD\nAQbfPUFwQjETHphH4XRr9GWMMefDU3EXkSXAr4EA8HtV/cmY5wuBp4ErgA7gTlU96DWIwd2n6P5r\nC/HBCOXXTyH0lalIvrUOMMaY85WwuItIAFgD3AC0ATtEpFlV944atgroUtUGEVkB/BS4M9G+y/ID\ndDyzl/CeDvIvKqX2vkspuKjs/DIxxhjzP14Oj68EWlS1VVVHgGeBZWPGLAP+6G6/ACySBFcWFQYK\naVzwCOGPOwktmc7E711mhd0YY5LES3GfDBwedb/Nfey0Y1Q1CvQANWfbaUmwnJGBo0z6wXxC102x\nDo7GGJNEaT2hKiKrgdXu3eHGtfd9yNr70hmC32qBU34HkWaW8/gw3nL2M99pXgZ5Ke5HgCmj7te5\nj51uTJuIBIEKnBOrn6Oq64B1ACKyU1UXeAkyV1jO44PlnPuyIV8vcyE7gJkicrGIFAArgOYxY5qB\ne93t5cDrqqrJC9MYY8y5SHjkrqpREXkYeBlnKeSTqrpHRJ4AdqpqM/AH4BkRaQE6cX4BGGOM8Ymn\nOXdV3QhsHPPY46O2h4Cvn+NrrzvH8bnAch4fLOfcl/H5is2eGGNM7rH1h8YYk4NSXtxFZImIfCwi\nLSLy6GmeLxSR59znt4nI9FTHlGoecn5ERPaKyC4ReU1EPC1tymSJch417g4RURHJ6JUGiXjJV0S+\n4b7Pe0TkT+mOMdk8fK6nisgbIvKe+9m+2Y84k0lEnhSRkyLy4RmeFxH5jfs92SUi89Md4xmpaspu\nOCdgPwHqgQLgA6BpzJjvAr9zt1cAz6UyplTfPOZ8PVDibj80HnJ2x5UDbwFbgQV+x53i93gm8B5Q\n5d6f6Hfcach5HfCQu90EHPQ77iTkfS0wH/jwDM/fDGwCBLga2OZ3zJ/dUn3knpLWBRkuYc6q+oaq\nDrp3t+JcO5DNvLzPAD/G6Ts0lM7gUsBLvvcDa1S1C0BVT6Y5xmTzkrMCIXe7AjiaxvhSQlXfwlkB\neCbLgKfVsRWoFJEL0xPd2aW6uKekdUGG85LzaKtwfvNns4Q5u3+uTlHVDekMLEW8vMeNQKOIbBGR\nrW5n1WzmJecfAStFpA1ndd330xOar8715z1tMqaf+3gkIiuBBcBCv2NJJRHJA34JfMvnUNIpiDM1\ncx3OX2ZvichcVe32NarUugt4SlV/ISLX4Fz7cqmqxv0ObDxK9ZH7ubQu4GytC7KIl5wRkcXAY8Ct\nqjqcpthSJVHO5cClwGYROYgzN9mcxSdVvbzHbUCzqkZU9QCwD6fYZysvOa8C/gKgqu8ARTg9WHKZ\np593P6S6uI/H1gUJcxaRy4G1OIU92+diIUHOqtqjqrWqOl1Vp+OcZ7hVVXf6E+7/zcvn+iWco3ZE\npBZnmqY1nUEmmZecPwUWAYjIbJzi3p7WKNOvGbjHXTVzNdCjqsf8DgpI7WqZUWeT9+GcaX/MfewJ\nnB9ucD4AzwMtwHag3u+zzGnI+VXgBPC+e2v2O+ZU5zxm7GayeLWMx/dYcKai9gK7gRV+x5yGnJuA\nLTgrad4Hvup3zEnI+c/AMSCC89fYKuBB4MFR7/Ma93uyO5M+13aFqjHG5CC7QtUYY3KQFXdjjMlB\nVtyNMSYHWXE3xpgcZMXdGGNykBV3Y4zJQVbcjTEmB1lxN8aYHPRf264/h9uIqYcAAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94-OCXRDP3cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etMgGdJjf2Tj",
        "colab_type": "code",
        "outputId": "ebbbc663-8afb-4a0c-e15a-44ce6e7ac1c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "brier_score(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "0.043720703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HLsqwEvahiy",
        "colab_type": "code",
        "outputId": "49d4a54f-5fba-4c6d-e773-e8fbacf53f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "    scores = f1(test_model, train_loader, avg = 'binary')\n",
        "    np.save(name + \"scores\", scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([16, 16])\n",
            "torch.Size([16, 16])\n",
            "(1, 256)\n",
            "set()\n",
            "178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1304\n",
            "1341\n",
            "1975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFLTv_en0WN3",
        "colab_type": "code",
        "outputId": "67d51452-b851-4a63-edfd-1e648da52a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "metrics(test_model, train_loader, name = name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 482095\n",
            "tp: 7520\n",
            "fn: 10875\n",
            "fp 11510\n",
            "prec: 0.39516552811350497\n",
            "rec: 0.408806740962218\n",
            "f1:  0.40187040748162994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiCx0wFnYxf",
        "colab_type": "code",
        "outputId": "5ba3f05b-63cf-40f1-96b0-cf590ab0cdc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36876444053062724"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe7BIv-liRj",
        "colab_type": "code",
        "outputId": "52191057-eafd-4b58-bfec-b6e64ead7569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import seaborn as sns\n",
        "plot = sns.distplot(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET8h8Kkd7X86",
        "colab_type": "code",
        "outputId": "e4ce27c8-3993-4803-cba1-a277991cf752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "plot.set_title(\"Histogram of average F1 Score per Image prediction\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Histogram of average F1 Score per Image prediction')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEHaB5b8EQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszwtyDuVvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, b in train_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK7oAiBWWX-2",
        "colab_type": "code",
        "outputId": "23f704f5-bffd-4a7d-aa97-9b693b8ad2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.count_nonzero(b) / (2000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.1975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwUwgS2W3GC",
        "colab_type": "text"
      },
      "source": [
        "# testing flipping the input direction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0nD-LmrX2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d, b in train_loader:\n",
        "    break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMHy4qTW7a9",
        "colab_type": "code",
        "outputId": "49ce9d9a-fb38-4988-82a4-f094a59da7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# set up some axes\n",
        "a = d\n",
        "a = a.numpy()\n",
        "c = np.flip(a, 1)\n",
        "\n",
        "# c = torch.flip(a, (0,1))\n",
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(c[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAOICAYAAAAU5r/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j08NdPNhZoJG",
        "colab_type": "code",
        "outputId": "98d28794-5212-44c8-e08f-6b5bc98fe5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "c.shape\n",
        "c = np.array(c)\n",
        "torch.tensor(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0032, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0026, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0147, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.1184,  0.1760],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0032],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0262, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.0262, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0493, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830]]]],\n",
              "\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0954],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0320, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6582, -0.6582, -0.6582,  ..., -0.1417, -0.1417, -0.7352],\n",
              "           [-0.4262, -0.4262, -0.4262,  ..., -0.0473, -0.0473, -0.2232],\n",
              "           [-0.4262, -0.4262, -0.4262,  ..., -0.0473, -0.0473, -0.2232],\n",
              "           ...,\n",
              "           [ 0.4872,  0.4872,  0.4872,  ..., -0.1823, -0.1823, -0.6837],\n",
              "           [ 0.4872,  0.4872,  0.4872,  ..., -0.1823, -0.1823, -0.6837],\n",
              "           [ 0.4872,  0.4872,  0.4872,  ..., -0.1823, -0.1823, -0.6837]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.1069],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.1242],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0089, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.4296],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.3547],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.1184],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084,  0.0320,  0.0781,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0723]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.0147, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084,  0.0896, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0089]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084,  0.0377,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084,  0.0262,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.0435, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]]]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY3GRCuQYkQZ",
        "colab_type": "code",
        "outputId": "cac64879-3fce-49ca-d518-fbece4b31f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "e.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-f73a2bc07fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBo6C1MAZdGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = torch.tensor(c)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSGYIr8YYKox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(e.numpy()[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwsDysmeW87k",
        "colab_type": "text"
      },
      "source": [
        "# after flipping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQF2DnA6yB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d) // 3\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "losses = batch_loss_histogram(test_model, train_loader, loss_func = c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ic2GUlrX799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sirv8aO61kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pbxmoet7IUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cPmIoZ3JNl",
        "colab_type": "text"
      },
      "source": [
        "## making histograms to check kernel size effect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEBDQBR3VKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as sns\n",
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d).to(device)\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHCoJCKbhiPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloIqpwpW6Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a,b in train_loader:\n",
        "    break\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbcXFd1pU6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = a.to(device)\n",
        "b = b.to(device)\n",
        "c(a[0][0][0],b[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIbwcFpW99P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b[0]\n",
        "# sdaddasdasadad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVeZgua3NNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = batch_loss_histogram(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYvtfNvMrMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEulvwY35_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change in all - train_index  = list\n",
        "\n",
        "\n",
        "\n",
        "# truth = train[:][1]\n",
        "truth.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAHdhEAmCAYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.application_boolean\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Eneh2CFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = train[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX4AYML9CG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[0][0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX25bMZtCS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[1]\n",
        "plt.imshow(ans[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqHhfnU1A8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = truth.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-lOg4RmBHkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR7kZg8BJc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[t>0] = 1\n",
        "t[t<0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdBvH9PE2G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ndgb4yy_4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incident_map = np.sum(t, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BoXcdbFD-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = sns.heatmap(incident_map).set_title(\"Total Number of UCDP Events in Training Set of 46898\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlhsFIUIdbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot_fig = heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"heatmap_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgD8oDV2LRJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3LIRp9NI0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors = (46898  - incident_map)// incident_map\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8uk9UI6hGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UQbluAaTTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO2uxF3LUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"weights_bce\", multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6MygGdJzni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_heatmap = sns.heatmap(multiplicative_factors)\n",
        "pyplot_fig = second_heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"multiplicative_factors_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Coy8QM7DA",
        "colab_type": "text"
      },
      "source": [
        "# applying weight function to lossy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htD4jvXUN-gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = torch.tensor(multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIF2EHZODYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func  = nn.BCEWithLogitsLoss(pos_weight= weights)\n",
        "loss_default = nn.BCEWithLogitsLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg-X4tXNanfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = b[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_KNeqBapXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d[1 > d] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhnCUQDawzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsFFKJ7WaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(a[0][-1][0],b[0]))\n",
        "print(loss_default(a[0][-1][0], b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEo_Gr8PXhS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.ones_like(a[0][-1][0])\n",
        "c *= -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8aP4eTX7cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizNoE4vXoGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(c,b[0]))\n",
        "print(loss_default(c, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyngsiPa5Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(d,b[0]))\n",
        "print(loss_default(d, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9AGiQKOePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = batch_loss_histogram(test_model, train_loader, loss_func)\n",
        "l2 = batch_loss_histogram(test_model, train_loader, loss_default)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-l3gnzjPGyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(l1)\n",
        "plt.figure()\n",
        "sns.distplot(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}